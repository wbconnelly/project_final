{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Data and read it into Pandas an merge the Company Sectors data with the financial \n",
    "# data based on company symbol\n",
    "\n",
    "fin_data = pd.read_csv(\"https://raw.githubusercontent.com/wbconnelly/Bill_Data_Sci/master/project%20draft/fin_data.csv\")\n",
    "company_sectors = pd.read_csv(\"https://raw.githubusercontent.com/wbconnelly/Bill_Data_Sci/master/project%20draft/company_sectors.csv\")\n",
    "company_sectors.rename(columns={'Symbol':'company_symbol'}, inplace = True)\n",
    "\n",
    "# Company data is the unaveraged dataset\n",
    "company_data = pd.merge(fin_data, company_sectors, on = 'company_symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the average of each column for each compnay over the five year period\n",
    "\n",
    "company_avg = company_data.groupby('company_symbol').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Explore features by observing which columns have the most missing values in each sector\n",
    "\n",
    "#creae an empty dictionary to store the data frames with the Sector null counts\n",
    "null_list = {}\n",
    "\n",
    "# add each to  data frame to see which columns for each sector are good candidates as predictors\n",
    "for sector in company_data.Sector.unique():\n",
    "    null_df = pd.DataFrame(company_data[company_data.Sector == sector].isnull().sum()).reset_index()\n",
    "    null_df.rename(columns = {0:'null_count', 'index':'col_title'}, inplace = True)\n",
    "    null_list[sector] = null_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete the unused columns from the list of features\n",
    "\n",
    "col_list_delete = ['extraordinaryitems',\n",
    "'deferredcharges',\n",
    "'accountingchange',\n",
    "'amended',\n",
    "'audited',\n",
    "'Unnamed: 0_y', 'preliminary',\n",
    "'Unnamed: 0_x',\n",
    " 'year',\n",
    " 'quarter',\n",
    " 'restated',\n",
    "'company_cik', \n",
    "'usdconversionrate',\n",
    "'periodlength',\n",
    "'original', 'crosscalculated', 'discontinuedoperations', \n",
    "'changeininventories', 'inventoriesnet',' otherequity', 'equityearnings']\n",
    "\n",
    "\n",
    "for col in col_list_delete:\n",
    "    try:    \n",
    "        feature_cols.remove(col)    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of sectors to iterate over\n",
    "sector_list = list(company_avg.Sector.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute all NaN values with the mean from it's respective sector\n",
    "\n",
    "feature_dfs = []\n",
    "for sector in sector_list:  \n",
    "    x = company_avg[company_avg.Sector == sector][feature_cols]\n",
    "    industry = pd.DataFrame(x.Sector, columns = ['Sector'])\n",
    "    x.drop('Sector', axis = 1, inplace= True)\n",
    "    x = x.fillna(x.mean())\n",
    "    #x = (x - x.mean())/ np.std(x)\n",
    "    #x = scaler.fit_transform(x)    \n",
    "    x = x.join(industry)\n",
    "    feature_dfs.append(x)\n",
    "    \n",
    "x = pd.concat(feature_dfs)\n",
    "x.reset_index(inplace = True)\n",
    "\n",
    "# Impute any remaining null values with the overall mean.\n",
    "x = x.fillna(x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an empty dictionary and fill it tables of the variance in each column for each sector\n",
    "\n",
    "var_list = {}\n",
    "for sector in sector_list:\n",
    "    var_tbl = pd.DataFrame(x[feature_cols][x.Sector == sector].var().reset_index())\n",
    "    var_tbl.rename(columns = {0:'variance', 'index':'col_title'}, inplace = True)\n",
    "    var_tbl.sort('variance', ascending = False, inplace = True)\n",
    "    var_tbl['total_var']= var_tbl['variance'].cumsum(skipna = True)/var_tbl.variance.sum()\n",
    "    var_list[sector] = var_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get dummy values for the Sectors\n",
    "y_vals = pd.get_dummies(x.Sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Center and scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x = scaler.fit_transform(x[feature_cols])\n",
    "x = pd.DataFrame(x, columns = feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create empty  dictionaries to contain the lists of features and the models for each sector, and instantiate a \n",
    "# Logistic Regression object\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_list = {}\n",
    "feature_lists = {}\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "\n",
    "# Run a for-loop that selects features based on the total variance limit in the variance list dictionary\n",
    "for sector in sector_list:\n",
    "    features = x[var_list[sector]['col_title'][var_list[sector].total_var <=.99999999999999999]].columns\n",
    "    x2 = x[feature_cols]\n",
    "    #x2 = x[feature_cols]    \n",
    "    feature_lists[sector] = features\n",
    "    feature_lists[sector] = feature_cols\n",
    "    y = y_vals[sector]    \n",
    "    model_list[sector] = logreg.fit(x2, y)\n",
    "    x2['predicted'] = model_list[sector].predict(x2)\n",
    "    print sector, '--- Accuracy',metrics.accuracy_score(y, x2.predicted), 'Number of features ', len(feature_lists[sector])\n",
    "    confusion = metrics.confusion_matrix(y, x2.predicted)\n",
    "    TP = confusion[1][1]\n",
    "    TN = confusion[0][0]\n",
    "    FP = confusion[0][1]\n",
    "    FN = confusion[1][0]\n",
    "    print 'True Positives:', TP\n",
    "    print 'True Negatives:', TN\n",
    "    print 'False Positives:', FP\n",
    "    print 'False Negatives:', FN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that will step though the variance limits for each sector\n",
    "def frange(init, end, step):\n",
    "    steps = []    \n",
    "    while init < end:\n",
    "        steps.append(init)\n",
    "        init += step\n",
    "    return steps\n",
    "\n",
    "\n",
    "#create empty dictionaries to record the performacne of the model for each sector at each variance limit\n",
    "false_pred_dict = {}\n",
    "num_feat_dict = {}\n",
    "var_limit_dict = {}\n",
    "accuracy_dict = {}\n",
    "\n",
    "\n",
    "# create a for-loop that records the model performance as the total variance in the model increases.\n",
    "\n",
    "for sector in sector_list:\n",
    "    false_pred = []\n",
    "    num_feat = []\n",
    "    var_limit = []\n",
    "    accuracy_list = []\n",
    "    for var in frange(0.5,1,.001):\n",
    "        \n",
    "        x2 = x[var_list[sector]['col_title'][var_list[sector].total_var <= var]]\n",
    "        y = y_vals[sector]\n",
    "        logreg.fit(x2, y)\n",
    "        x_pred = logreg.predict(x2)\n",
    "        accuracy = metrics.accuracy_score(y, x_pred)\n",
    "        confusion = metrics.confusion_matrix(y, x_pred)\n",
    "        TP = confusion[1][1]\n",
    "        TN = confusion[0][0]\n",
    "        FP = confusion[0][1]\n",
    "        FN = confusion[1][0]\n",
    "        total_false = FP + FN\n",
    "        false_pred.append(total_false)\n",
    "        var_limit.append(var)\n",
    "        num_feat.append(len(x2.columns))\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "        false_pred_dict[sector] = false_pred\n",
    "        num_feat_dict[sector] = num_feat\n",
    "        var_limit_dict[sector] = var_limit\n",
    "        accuracy_dict[sector] = accuracy_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the data for each sector model\n",
    "\n",
    "'Cyclical Consumer Goods & Services',\n",
    " 'Telecommunications Services',\n",
    " 'Industrials',\n",
    " 'Energy',\n",
    " 'None Found',\n",
    " 'Utilities',\n",
    " 'Financials',\n",
    " 'Healthcare',\n",
    " 'Basic Materials',\n",
    " 'Technology',\n",
    " 'Non-Cyclical Consumer Goods & Services'\n",
    "\n",
    "sector = 'Cyclical Consumer Goods & Services'\n",
    "plt.scatter(x = num_feat_dict[sector], y = var_limit_dict[sector])\n",
    "plt.scatter(x = num_feat_dict[sector], y = false_pred_dict[sector])\n",
    "plt.scatter(x = var_limit_dict[sector], y = false_pred_dict[sector])\n",
    "plt.scatter(x = var_limit_dict[sector], y = accuracy_dict[sector])\n",
    "plt.scatter(x = num_feat_dict[sector], y = accuracy_dict[sector])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------- test the models ---------------------#\n",
    "\n",
    "# Read in the test data and then center and scale it\n",
    "\n",
    "x_test = pd.read_csv('https://raw.githubusercontent.com/wbconnelly/Bill_Data_Sci/master/project%20draft/x_test.csv')\n",
    "y_test = pd.read_csv('https://raw.githubusercontent.com/wbconnelly/Bill_Data_Sci/master/project%20draft/y_test.csv')\n",
    "\n",
    "x_test = scaler.fit_transform(x_test[feature_cols])\n",
    "x_test = pd.DataFrame(x_test, columns = feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a for-loop that uses ensembling to test make predictions on the testing data.\n",
    "\n",
    "pred_list = []\n",
    "for sector in sector_list:\n",
    "    try:\n",
    "        x_cols = feature_lists[sector]\n",
    "        y = y_test[sector]\n",
    "\n",
    "# Use ensembling data with 100 iterations\n",
    "        for i in range(1,1000):\n",
    "            y_pred = model_list[sector].predict(x_test[x_cols])\n",
    "            pred_list.append(np.array(y_pred))\n",
    "            y_pred = np.round(np.mean(pred_list, axis = 0)).astype(int)\n",
    "        print sector, '---',metrics.accuracy_score(y, y_pred), \"number of features \", len(x_cols)\n",
    "        confusion = metrics.confusion_matrix(y, y_pred)\n",
    "        TP = confusion[1][1]\n",
    "        TN = confusion[0][0]\n",
    "        FP = confusion[0][1]\n",
    "        FN = confusion[1][0]\n",
    "        print 'True Positives:', TP\n",
    "        print 'True Negatives:', TN\n",
    "        print 'False Positives:', FP\n",
    "        print 'False Negatives:', FN\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
